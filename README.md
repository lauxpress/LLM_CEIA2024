# Fine-Tuning GPT-2 with Edgar Allan Poe's Short Stories

![images/header_image.png](https://github.com/lauxpress/LLM_CEIA2024/blob/main/EALLMPoe.jpg)

## Descripción

Este repositorio contiene el código y el informe del proyecto para el ajuste fino del modelo GPT-2 utilizando cuentos cortos de Edgar Allan Poe. El objetivo de este proyecto es adaptar el modelo preentrenado para generar texto con un estilo literario similar al del autor.

## Contenidos

- `Informe LLM.pdf/`: Informe del trabajo con todos los pasos detallados y los resultados obtenidos.
- `TF_LLM_2024_Scheihing.ipynb/`: Colab con los codigos utilizados para entrenar el modelo GPT-2.
- `EALLMPoe.jpg/`: Imagen de portada.

## Requisitos

- Python 3.7 o superior
- TensorFlow
- PyTorch
- Transformers (Hugging Face)
- Pandas
- Google Colab (opcional, para entrenamiento con GPU)
